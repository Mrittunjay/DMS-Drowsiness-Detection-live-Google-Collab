{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMpY4cR5veppOZyvDWsAea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":28,"metadata":{"id":"7bdOorP6Z3gt","executionInfo":{"status":"ok","timestamp":1705042029404,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"outputs":[],"source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64encode, b64decode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","from IPython.display import HTML\n","import json"]},{"cell_type":"code","source":["# Importing yolov5 object detection model\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n","!git clone https://github.com/ultralytics/yolov5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYLsWMw3nS_4","executionInfo":{"status":"ok","timestamp":1705040097005,"user_tz":-330,"elapsed":19527,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"0d2a213a-862c-47b1-f9f2-acb040449010"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu117\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Cloning into 'yolov5'...\n","remote: Enumerating objects: 16283, done.\u001b[K\n","remote: Counting objects: 100% (180/180), done.\u001b[K\n","remote: Compressing objects: 100% (147/147), done.\u001b[K\n","remote: Total 16283 (delta 76), reused 93 (delta 33), pack-reused 16103\u001b[K\n","Receiving objects: 100% (16283/16283), 15.10 MiB | 19.98 MiB/s, done.\n","Resolving deltas: 100% (11111/11111), done.\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnPjExK_oaCl","executionInfo":{"status":"ok","timestamp":1705040101130,"user_tz":-330,"elapsed":23,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"876f620f-b92b-456b-a281-b59745bcfe5c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34myolov5\u001b[0m/\n"]}]},{"cell_type":"code","source":["cd /content/yolov5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XN22t7qvofmH","executionInfo":{"status":"ok","timestamp":1705040104541,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"0f429e99-5048-41a3-f85f-57ed602d1c3a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrsmA3pSonZB","executionInfo":{"status":"ok","timestamp":1705040108407,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"6b834455-cb47-495f-e049-549584766358"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["benchmarks.py    \u001b[0m\u001b[01;34mdata\u001b[0m/       LICENSE         README.zh-CN.md   tutorial.ipynb\n","CITATION.cff     detect.py   \u001b[01;34mmodels\u001b[0m/         requirements.txt  \u001b[01;34mutils\u001b[0m/\n","\u001b[01;34mclassify\u001b[0m/        export.py   pyproject.toml  \u001b[01;34msegment\u001b[0m/          val.py\n","CONTRIBUTING.md  hubconf.py  README.md       train.py\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"X4fcB4uwoq14","executionInfo":{"status":"ok","timestamp":1705040126096,"user_tz":-330,"elapsed":15351,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"ea94e417-5864-4f48-cf08-9e3af8f44fb9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gitpython>=3.1.30 (from -r requirements.txt (line 5))\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n","Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.8.0.76)\n","Collecting Pillow>=10.0.1 (from -r requirements.txt (line 9))\n","  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.11.4)\n","Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.1)\n","Collecting ultralytics>=8.0.232 (from -r requirements.txt (line 18))\n","  Downloading ultralytics-8.1.0-py3-none-any.whl (699 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m699.2/699.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.12.2)\n","Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (67.7.2)\n","Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r requirements.txt (line 5))\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.1.0)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.232->-r requirements.txt (line 18)) (9.0.0)\n","Collecting hub-sdk>=0.0.2 (from ultralytics>=8.0.232->-r requirements.txt (line 18))\n","  Downloading hub_sdk-0.0.3-py3-none-any.whl (37 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.3.post1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5))\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n","Installing collected packages: smmap, Pillow, hub-sdk, gitdb, thop, gitpython, ultralytics\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pillow-10.2.0 gitdb-4.0.11 gitpython-3.1.41 hub-sdk-0.0.3 smmap-5.0.1 thop-0.1.1.post2209072238 ultralytics-8.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{}}]},{"cell_type":"code","source":["cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lry2b5owpLt9","executionInfo":{"status":"ok","timestamp":1705040138869,"user_tz":-330,"elapsed":335,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"0f729736-a0b9-4e3d-dbee-62259d4436e8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBn8aXqip5UF","executionInfo":{"status":"ok","timestamp":1705040150888,"user_tz":-330,"elapsed":9432,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"dea88168-4a0d-46b0-d190-f923c368b76a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.1.0)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.2.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: hub-sdk>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"]}]},{"cell_type":"code","source":["import torch\n","from matplotlib import pyplot as plt"],"metadata":{"id":"mdgNv5edqY95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ak1618CQp_0L","executionInfo":{"status":"ok","timestamp":1705036020411,"user_tz":-330,"elapsed":9500,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"fc726ddb-1806-4878-8fc0-0f3733f91fb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","YOLOv5 ðŸš€ 2024-1-12 Python-3.10.12 torch-2.1.0+cu121 CPU\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 110MB/s] \n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","Adding AutoShape... \n"]}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0XWz68vqfZP","executionInfo":{"status":"ok","timestamp":1705036024343,"user_tz":-330,"elapsed":406,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"9d5eb74e-1329-4368-9b40-01cc65567416"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AutoShape(\n","  (model): DetectMultiBackend(\n","    (model): DetectionModel(\n","      (model): Sequential(\n","        (0): Conv(\n","          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n","          (act): SiLU(inplace=True)\n","        )\n","        (1): Conv(\n","          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (2): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (3): Conv(\n","          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (4): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (5): Conv(\n","          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (6): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (7): Conv(\n","          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (8): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (9): SPPF(\n","          (cv1): Conv(\n","            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","        )\n","        (10): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (11): Upsample(scale_factor=2.0, mode='nearest')\n","        (12): Concat()\n","        (13): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (14): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (15): Upsample(scale_factor=2.0, mode='nearest')\n","        (16): Concat()\n","        (17): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (18): Conv(\n","          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (19): Concat()\n","        (20): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (21): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (22): Concat()\n","        (23): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (24): Detect(\n","          (m): ModuleList(\n","            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n","            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# Testing the pretrainid model\n","img = \"https://s.w-x.co/in-kittens.jpg\"\n","result = model(img)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"fkLsVddSqlJx","executionInfo":{"status":"error","timestamp":1705036034483,"user_tz":-330,"elapsed":1139,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"fbebb9a3-677e-4296-f5eb-4dd03b2dd4bc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"UnidentifiedImageError","evalue":"cannot identify image file <_io.BytesIO object at 0x78ec01362a20>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-814e764ae5e7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing the pretrainid model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://s.w-x.co/in-kittens.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"image{i}\"\u001b[0m  \u001b[0;31m# filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filename or uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                     \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexif_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3283\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mexclusive_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3284\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x78ec01362a20>"]}]},{"cell_type":"code","source":["%matplotlib inline\n","plt.imshow(np.squeeze(result.render()))\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212},"id":"2MsU9LNKqwmx","executionInfo":{"status":"error","timestamp":1705036042835,"user_tz":-330,"elapsed":346,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"b462405d-31bb-4355-a928-eb528bf01767"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'result' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-39dd0c575a3c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"]}]},{"cell_type":"code","source":["# Getting the facial landmark data\n","!wget   http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-GI-phmOq7Cp","executionInfo":{"status":"ok","timestamp":1705040161697,"user_tz":-330,"elapsed":2618,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"9f32be28-d16a-43ba-bac1-a90e89e133ce"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-01-12 06:15:58--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","Resolving dlib.net (dlib.net)... 107.180.26.78\n","Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 64040097 (61M)\n","Saving to: â€˜shape_predictor_68_face_landmarks.dat.bz2â€™\n","\n","shape_predictor_68_ 100%[===================>]  61.07M  26.7MB/s    in 2.3s    \n","\n","2024-01-12 06:16:01 (26.7 MB/s) - â€˜shape_predictor_68_face_landmarks.dat.bz2â€™ saved [64040097/64040097]\n","\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vD6T_S6FrJ1i","executionInfo":{"status":"ok","timestamp":1705040166953,"user_tz":-330,"elapsed":512,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"0ad5932c-17e9-4c6b-82d7-e6a32f842169"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/  shape_predictor_68_face_landmarks.dat.bz2  \u001b[01;34myolov5\u001b[0m/\n"]}]},{"cell_type":"code","source":["!bunzip2 /content/shape_predictor_68_face_landmarks.dat.bz2"],"metadata":{"id":"1zPFcMJnrNUD","executionInfo":{"status":"ok","timestamp":1705040179478,"user_tz":-330,"elapsed":7819,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBTbW2RErTdf","executionInfo":{"status":"ok","timestamp":1705040183358,"user_tz":-330,"elapsed":534,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}},"outputId":"78a29287-7488-4fb5-8216-135ee0f91ba7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/  shape_predictor_68_face_landmarks.dat  \u001b[01;34myolov5\u001b[0m/\n"]}]},{"cell_type":"code","source":["# dlib for deep learning based modules and face landmark detection\n","import dlib\n","# face_utils for basic operations of convolutions\n","from imutils import face_utils"],"metadata":{"id":"9PfJzd3yr6rQ","executionInfo":{"status":"ok","timestamp":1705040189581,"user_tz":-330,"elapsed":1051,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# initializing the face and landmark detectors\n","detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor(\"/content/shape_predictor_68_face_landmarks.dat\")\n","\n","# status marking for current state\n","sleep = 0\n","drowsy = 0\n","active = 0\n","# status = 0\n","# color = (0,0,0)"],"metadata":{"id":"O-SCfzXFruc2","executionInfo":{"status":"ok","timestamp":1705040196361,"user_tz":-330,"elapsed":2330,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#NOT WORKING DISCARD THIS CODE\n","# Calculating change in landmark points around the eyes\n","def compute(ptA, ptB):\n","  dist = np.linalg.norm(ptA - ptB)\n","  return dist\n","\n","def blinked(a,b,c,d,e,f):\n","  up = compute(a,b) + compute(c,e)\n","  down = compute(a,f)\n","  ratio = up / (2.0*down)\n","\n","  # print(\"\\nDistances: \", compute(a,b), compute(c,e), compute(a,f))\n","  print(\"\\nRatio: \\n\",ratio)\n","\n","  # Checking if it is blinked\n","  if(ratio>0.25):\n","    return 2\n","  elif(ratio>0.21 and ratio<=0.25):\n","    return 1\n","  else:\n","    return 0"],"metadata":{"id":"xQv7Tc2zsHr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","\"\"\"\n","Params:\n","        js_reply: JavaScript object containing image from webcam\n","Returns:\n","        img: OpenCV BGR image\n","\"\"\"\n","def js_to_image(js_reply):\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","\"\"\"\n","Params:\n","        bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","Returns:\n","      bytes: Base64 image byte string\n","\"\"\"\n","def bbox_to_bytes(bbox_array):\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","  return bbox_bytes\n","\n","def landmarks_to_bytes(landmarks):\n","  # Create an image with a transparent background\n","  img = PIL.Image.new('RGBA', (640, 480), (0, 0, 0, 0))\n","  draw = PIL.ImageDraw.Draw(img)\n","  # Draw dots on the image for each facial landmark\n","  for landmark in landmarks:\n","      draw.ellipse([landmark[0] - 2, landmark[1] - 2, landmark[0] + 2, landmark[1] + 2], fill='blue', outline='white')\n","  # Convert image to bytes\n","  img_bytes = io.BytesIO()\n","  img.save(img_bytes, format='png')\n","  # Format as base64 string\n","  landmarks_bytes = 'data:image/png;base64,{}'.format(str(b64encode(img_bytes.getvalue()), 'utf-8'))\n","  return landmarks_bytes"],"metadata":{"id":"DJFeWEYla0yJ","executionInfo":{"status":"ok","timestamp":1705040210498,"user_tz":-330,"elapsed":334,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### **CODE WITH FACE DETECTED AND BOX DRAWN**"],"metadata":{"id":"CCl5FBUv5Hjt"}},{"cell_type":"code","source":["# HTML code for canvas element\n","html_code = '''\n","<canvas id=\"canvasElement\" width=\"640\" height=\"480\"></canvas>\n","'''\n","\n","# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    /*\n","    Text Printing\n","    */\n","    var textElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      /*\n","      Text Printing\n","      */\n","      // Create a new div for displaying text\n","      textElement = document.createElement('div');\n","      textElement.style.position = 'absolute';\n","      textElement.style.zIndex = 2;\n","      textElement.style.color = 'Green';\n","      textElement.style.fontWeight = 'bold';\n","      textElement.style.fontSize = '20px';\n","      textElement.style.top = '100px';  // Set the top position\n","      textElement.style.left = '100px';  // Set the left position\n","      div.appendChild(textElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      /*\n","      Used for drawing onscreen rectanble\n","      not in use now\n","      */\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      // Get the canvas element (Draw dots on the canvas for facial landmarks)\n","      var canvasElement = document.getElementById('canvasElement');\n","      var context = canvasElement.getContext('2d');\n","\n","      // Draw dots on the canvas for facial landmarks\n","      context.clearRect(0, 0, canvasElement.width, canvasElement.height);\n","\n","      // if (landmarks && landmarks.length > 0) {\n","      //   for (let landmark of landmarks) {\n","      //     context.beginPath();\n","      //     context.arc(landmark.x, landmark.y, 2, 0, 2 * Math.PI, false);\n","      //     context.fillStyle = 'blue'; // You can set the dot color\n","      //     context.fill();\n","      //     context.lineWidth = int(1);\n","      //     context.strokeStyle = 'white'; // You can set the dot border color\n","      //     context.stroke();\n","      //   }\n","      // }\n","\n","      /*\n","      Text Printing\n","      */\n","      // Set text content for the text element\n","      textElement.innerText = \"Your Text Here\";\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(HTML(html_code))\n","  display(js)\n","\n","def video_frame(label, imgData):\n","  # data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\")'.format(label, bbox, landmarks))\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, imgData))\n","  return data"],"metadata":{"id":"sYuZEx8Ha-_S","executionInfo":{"status":"ok","timestamp":1705063281855,"user_tz":-330,"elapsed":525,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["# # initialize the Haar Cascade face detection model\n","# face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"],"metadata":{"id":"t-v5YEukoPGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","bbox_array = ''\n","count = 0\n","color = (0, 0, 0) #Color red\n","landmarks_J = ''\n","\n","while True:\n","    # js_reply = video_frame(label, bbox, landmarks_J)\n","    js_reply = video_frame(label, bbox)\n","\n","    if not js_reply:\n","      break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # Add text to the image\n","\n","    # This method of printing text didnot work here have to implement\n","    # the text printing using the javascript module\n","    # text = \"Hello World!\"\n","    # cv2.putText(img, text, (200, 200), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n","\n","    faces = detector(gray)\n","    #detected face in faces array\n","    for face in faces:\n","        x1 = face.left()\n","        y1 = face.top()\n","        x2 = face.right()\n","        y2 = face.bottom()\n","\n","        bbox_array = cv2.rectangle(bbox_array,(x1,y1),(x2, y2),(0,255,0),2)\n","\n","        # Not required because the rectangle is drawn in javascript code\n","        # face_frame = img.copy()\n","        # cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","\n","        landmarks = predictor(gray, face)\n","        landmarks = face_utils.shape_to_np(landmarks)\n","\n","        # Eye landmarks are as follows:\n","        # Referenced from https://www.studytonight.com/post/dlib-68-points-face-landmark-detection-with-opencv-and-python\n","        left_blink = blinked(landmarks[37],landmarks[38],\n","          landmarks[39], landmarks[42], landmarks[41], landmarks[40])\n","        right_blink = blinked(landmarks[43],landmarks[44],\n","          landmarks[45], landmarks[48], landmarks[47], landmarks[46])\n","\n","        # Detecting behaviour according to eye blink value\n","        if(left_blink == 0 or right_blink == 0):\n","          sleep+=1\n","          drowsy = 0\n","          active = 0\n","          if(sleep > 6):\n","            status = \"SLEEPING !!!\"\n","            color = (255,0,0)\n","        elif(left_blink == 1 or right_blink == 1):\n","          sleep = 0\n","          active = 0\n","          drowsy += 1\n","          if(drowsy > 6):\n","            status = \"Drowsy !\"\n","            colour = (0,0.255)\n","        else:\n","          drowsy = 0\n","          sleep = 0\n","          active += 1\n","          if(active>6):\n","            status = \"Active :)\"\n","            color = (0,255,0)\n","\n","    # # get face region coordinates\n","    # faces = face_cascade.detectMultiScale(gray)\n","\n","    # # get face bounding box for overlay\n","    # for (x,y,w,h) in faces:\n","    #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","\n","    # # updating landmarks_J for new frame\n","    # landmarks = landmarks_to_bytes(landmarks)\n","    # landmarks_J = landmarks\n","\n"],"metadata":{"id":"I5uBo3cibCi9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4GGsV2XElhyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C7zvRxJwliRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CODE WITH STATUS TEXT PRINTED ON VIDEO STREAM**"],"metadata":{"id":"v5Vy0HtX5Y48"}},{"cell_type":"code","source":["from scipy.spatial import distance as dist"],"metadata":{"id":"iXNlrr_IIvti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eye_aspect_ratio(eye):\n","    # Calculate the Euclidean distances between the two sets of vertical eye landmarks\n","    A = dist.euclidean(eye[1], eye[5])\n","    B = dist.euclidean(eye[2], eye[4])\n","\n","    # Calculate the Euclidean distance between the horizontal eye landmarks\n","    C = dist.euclidean(eye[0], eye[3])\n","\n","    # Compute the eye aspect ratio\n","    ear = (A + B) / (2.0 * C)\n","\n","    return ear"],"metadata":{"id":"0F12BtLiIyRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HTML code for canvas element\n","html_code = '''\n","<canvas id=\"canvasElement\" width=\"640\" height=\"480\"></canvas>\n","'''\n","\n","# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    /*\n","    Text Printing\n","    */\n","    var textElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      /*\n","      Text Printing\n","      */\n","      // Create a new div for displaying text\n","      textElement = document.createElement('div');\n","      textElement.style.position = 'absolute';\n","      textElement.style.zIndex = 2;\n","      // textElement.style.color = 'Green';\n","      textElement.style.fontWeight = 'bold';\n","      textElement.style.fontSize = '20px';\n","      // textElement.style.top = '100px';  // Set the top position\n","      // textElement.style.left = '100px';  // Set the left position\n","      div.appendChild(textElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, imgData, status) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      /*\n","      Used for drawing onscreen rectanble\n","      not in use now\n","      */\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      // Get the canvas element (Draw dots on the canvas for facial landmarks)\n","      var canvasElement = document.getElementById('canvasElement');\n","      var context = canvasElement.getContext('2d');\n","\n","      // Draw dots on the canvas for facial landmarks\n","      context.clearRect(0, 0, canvasElement.width, canvasElement.height);\n","\n","      // if (landmarks && landmarks.length > 0) {\n","      //   for (let landmark of landmarks) {\n","      //     context.beginPath();\n","      //     context.arc(landmark.x, landmark.y, 2, 0, 2 * Math.PI, false);\n","      //     context.fillStyle = 'blue'; // You can set the dot color\n","      //     context.fill();\n","      //     context.lineWidth = int(1);\n","      //     context.strokeStyle = 'white'; // You can set the dot border color\n","      //     context.stroke();\n","      //   }\n","      // }\n","\n","      /*\n","      Text Printing\n","      */\n","      // Set text content for the text element\n","      if (status != null) {\n","        var videoRect = video.getClientRects()[0];\n","        textElement.style.top = videoRect.top + \"px\";//'100px';  // Set the top position\n","        textElement.style.left = videoRect.left + \"px\";//'100px';  // Set the left position\n","        if (status == 1) {\n","          textElement.innerText = \"Active :)\"\n","          textElement.style.color = 'Green';\n","        } else if (status == 2) {\n","          textElement.innerText = \"Drowsy !!!\"\n","          textElement.style.color = 'Red';\n","        } else if (status == 3) {\n","          textElement.innerText = \"Uncertain\"\n","          textElement.style.color = 'Yellow';\n","        } else {\n","          textElement.innerText = \"IDLE\";\n","          textElement.style.color = 'Yellow';\n","        }\n","      }\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(HTML(html_code))\n","  display(js)\n","\n","def video_frame(label, imgData, status):\n","  # data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\")'.format(label, bbox, landmarks))\n","  data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\")'.format(label, imgData, status))\n","  return data"],"metadata":{"id":"g7gxhesGlmnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","bbox_array = ''\n","count = 0\n","color = (0, 0, 0) #Color red\n","landmarks_J = ''\n","status = 0\n","current_status = 0\n","left_thresh = 3\n","right_thresh = 5\n","\n","while True:\n","    # js_reply = video_frame(label, bbox, landmarks_J)\n","    js_reply = video_frame(label, bbox, status)\n","\n","    if not js_reply:\n","      break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # Add text to the image\n","\n","    # This method of printing text didnot work here have to implement\n","    # the text printing using the javascript module\n","    # text = \"Hello World!\"\n","    # cv2.putText(img, text, (200, 200), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n","\n","    faces = detector(gray)\n","    #detected face in faces array\n","    for face in faces:\n","        x1 = face.left()\n","        y1 = face.top()\n","        x2 = face.right()\n","        y2 = face.bottom()\n","\n","        bbox_array = cv2.rectangle(bbox_array,(x1,y1),(x2, y2),(0,255,0),2)\n","\n","        # Not required because the rectangle is drawn in javascript code\n","        # face_frame = img.copy()\n","        # cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","\n","        landmarks = predictor(gray, face)\n","        landmarks = face_utils.shape_to_np(landmarks)\n","\n","        # Eye landmarks are as follows:\n","        # Referenced from https://www.studytonight.com/post/dlib-68-points-face-landmark-detection-with-opencv-and-python\n","        left_blink = eye_aspect_ratio([landmarks[37],landmarks[38],\n","          landmarks[39], landmarks[40], landmarks[41], landmarks[42]])\n","        # print(\"left_blink :\", left_blink)\n","        right_blink = eye_aspect_ratio([landmarks[43],landmarks[44],\n","          landmarks[45], landmarks[46], landmarks[47], landmarks[48]])\n","        # print(\"right_blink: \", right_blink)\n","\n","        # Classify state based on EAR thresholds\n","        if left_blink < left_thresh and right_blink < right_thresh:\n","            current_status = 1 #\"Active\"\n","        elif left_blink > left_thresh and right_blink > right_thresh:\n","            current_status = 2 #\"Drowsy\"\n","        else:\n","            current_status = 3 #\"Uncertain\"\n","\n","    # # get face region coordinates\n","    # faces = face_cascade.detectMultiScale(gray)\n","\n","    # # get face bounding box for overlay\n","    # for (x,y,w,h) in faces:\n","    #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","    status = current_status\n","\n","\n","    # # updating landmarks_J for new frame\n","    # landmarks = landmarks_to_bytes(landmarks)\n","    # landmarks_J = landmarks\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":501},"id":"SM2n-sIs5gW1","outputId":"64edfa8f-ac42-4d34-e9cf-f55877197fa3"},"execution_count":null,"outputs":[{"data":{"text/html":["\n","<canvas id=\"canvasElement\" width=\"640\" height=\"480\"></canvas>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    /*\n","    Text Printing\n","    */\n","    var textElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      /*\n","      Text Printing\n","      */\n","      // Create a new div for displaying text\n","      textElement = document.createElement('div');\n","      textElement.style.position = 'absolute';\n","      textElement.style.zIndex = 2;\n","      // textElement.style.color = 'Green';\n","      textElement.style.fontWeight = 'bold';\n","      textElement.style.fontSize = '20px';\n","      // textElement.style.top = '100px';  // Set the top position\n","      // textElement.style.left = '100px';  // Set the left position\n","      div.appendChild(textElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, imgData, status) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      /*\n","      Used for drawing onscreen rectanble\n","      not in use now\n","      */\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      // Get the canvas element (Draw dots on the canvas for facial landmarks)\n","      var canvasElement = document.getElementById('canvasElement');\n","      var context = canvasElement.getContext('2d');\n","\n","      // Draw dots on the canvas for facial landmarks\n","      context.clearRect(0, 0, canvasElement.width, canvasElement.height);\n","\n","      // if (landmarks && landmarks.length > 0) {\n","      //   for (let landmark of landmarks) {\n","      //     context.beginPath();\n","      //     context.arc(landmark.x, landmark.y, 2, 0, 2 * Math.PI, false);\n","      //     context.fillStyle = 'blue'; // You can set the dot color\n","      //     context.fill();\n","      //     context.lineWidth = int(1);\n","      //     context.strokeStyle = 'white'; // You can set the dot border color\n","      //     context.stroke();\n","      //   }\n","      // }\n","\n","      /*\n","      Text Printing\n","      */\n","      // Set text content for the text element\n","      if (status != null) {\n","        var videoRect = video.getClientRects()[0];\n","        textElement.style.top = videoRect.top + \"px\";//'100px';  // Set the top position\n","        textElement.style.left = videoRect.left + \"px\";//'100px';  // Set the left position\n","        if (status == 1) {\n","          textElement.innerText = \"Active :)\"\n","          textElement.style.color = 'Green';\n","        } else if (status == 2) {\n","          textElement.innerText = \"Drowsy !!!\"\n","          textElement.style.color = 'Red';\n","        } else if (status == 3) {\n","          textElement.innerText = \"Uncertain\"\n","          textElement.style.color = 'Yellow';\n","        } else {\n","          textElement.innerText = \"IDLE\";\n","          textElement.style.color = 'Yellow';\n","        }\n","      }\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":[],"metadata":{"id":"-AIVkSwJY17J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PqBnrmsXY1nu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CODE WITH STATUS TEXT PRINTED ON VIDEO STREAM WITH 68 PLOTTED POINTS**"],"metadata":{"id":"q-wfQCvUY5W7"}},{"cell_type":"code","source":["from scipy.spatial import distance as dist"],"metadata":{"id":"_Fnbvut4ZPVJ","executionInfo":{"status":"ok","timestamp":1705040229108,"user_tz":-330,"elapsed":391,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def eye_aspect_ratio(eye):\n","    # Calculate the Euclidean distances between the two sets of vertical eye landmarks\n","    A = dist.euclidean(eye[1], eye[5])\n","    B = dist.euclidean(eye[2], eye[4])\n","\n","    # Calculate the Euclidean distance between the horizontal eye landmarks\n","    C = dist.euclidean(eye[0], eye[3])\n","\n","    # Compute the eye aspect ratio\n","    ear = (A + B) / (2.0 * C)\n","\n","    return ear"],"metadata":{"id":"BFlwPyNfY9KE","executionInfo":{"status":"ok","timestamp":1705040231981,"user_tz":-330,"elapsed":410,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# HTML code for canvas element\n","html_code = '''\n","<canvas id=\"canvasElement\" width=\"640\" height=\"480\"></canvas>\n","'''\n","\n","# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    /*\n","    Text Printing\n","    */\n","    var textElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      /*\n","      Text Printing\n","      */\n","      // Create a new div for displaying text\n","      textElement = document.createElement('div');\n","      textElement.style.position = 'absolute';\n","      textElement.style.zIndex = 2;\n","      // textElement.style.color = 'Green';\n","      textElement.style.fontWeight = 'bold';\n","      textElement.style.fontSize = '40px';\n","      // textElement.style.top = '100px';  // Set the top position\n","      // textElement.style.left = '100px';  // Set the left position\n","      div.appendChild(textElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, status, imgData, landmarks) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      /*\n","      Used for drawing onscreen rectanble\n","      */\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      // Get the canvas element (Draw dots on the canvas for facial landmarks)\n","      var canvasElement = document.getElementById('canvasElement');\n","      canvasElement.style.position = 'absolute';\n","      canvasElement.style.top = '0';\n","      canvasElement.style.left = '0';\n","      var context = canvasElement.getContext('2d');\n","\n","      // Draw dots on the canvas for facial landmarks\n","      context.clearRect(0, 0, canvasElement.width, canvasElement.height);\n","\n","      var sub_arrays = landmarks.slice(1, -1).split(\"], [\");\n","      var array_js = sub_arrays.map(subarray => subarray.split(',').map(Number));\n","      // var length_js = array_js.length;\n","      if (array_js && array_js.length > 0) {\n","        for (let [x, y] of array_js) {\n","          context.beginPath();\n","          // x and y values are manipulate to fine tune the plotting of the facial points in javascript canvus\n","          context.arc(x-10, y+14, 2, 0, 2 * Math.PI, false);\n","          context.fillStyle = 'green'; // You can set the dot color\n","          context.fill();\n","          context.lineWidth = 1;\n","          context.strokeStyle = 'black'; // You can set the dot border color\n","          context.stroke();\n","        }\n","      }\n","\n","      /*\n","      Text Printing\n","      */\n","      // Set text content for the text element\n","      if (status != null) {\n","        var videoRect = video.getClientRects()[0];\n","        textElement.style.top = videoRect.top + \"px\";//'100px';  // Set the top position\n","        textElement.style.left = videoRect.left + \"px\";//'100px';  // Set the left position\n","        if (status == 1) {\n","          textElement.innerText = \"Active :)\"\n","          textElement.style.color = 'Green';\n","          textElement.style.border = '2px solid white';\n","        } else if (status == 2) {\n","          textElement.innerText = \"Drowsy !!!\"\n","          textElement.style.color = 'Red';\n","          textElement.style.border = '2px solid red';\n","        } else if (status == 3) {\n","          textElement.innerText = \"Uncertain\"\n","          textElement.style.color = 'Yellow';\n","          textElement.style.border = '2px solid yellow';\n","        } else {\n","          textElement.innerText = \"IDLE\";\n","          textElement.style.color = 'Yellow';\n","          textElement.style.border = '2px solid yellow';\n","        }\n","      }\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(HTML(html_code))\n","  display(js)\n","\n","def video_frame(label, status, bbox, landmarks):\n","  data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\", \"{}\")'.format(label, status, bbox, landmarks))\n","  return data"],"metadata":{"id":"7prU35SVZRGV","executionInfo":{"status":"ok","timestamp":1705068830561,"user_tz":-330,"elapsed":673,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":210,"outputs":[]},{"cell_type":"code","source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","bbox_array = ''\n","count = 0\n","color = (0, 0, 0) #Color red\n","landmarks_J = {}\n","status = 0\n","current_status = 0\n","left_thresh = 3\n","right_thresh = 5\n","\n","while True:\n","    # js_reply = video_frame(label, bbox, landmarks_J)\n","    js_reply = video_frame(label, status, bbox, landmarks_J)\n","\n","    if not js_reply:\n","      break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # Add text to the image\n","\n","    # This method of printing text didnot work here in google collab\n","    # So, had to implement the text printing using the javascript module\n","    # text = \"Hello World!\"\n","    # cv2.putText(img, text, (200, 200), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n","\n","    faces = detector(gray)\n","    #detected face in faces array\n","    for face in faces:\n","        x1 = face.left()\n","        y1 = face.top()\n","        x2 = face.right()\n","        y2 = face.bottom()\n","        # x and y values are manipulate to increase the size of the bounding box\n","        bbox_array = cv2.rectangle(bbox_array,(x1-40,y1-40),(x2+40, y2+40),(0,255,0),1)\n","\n","        # Not required because the rectangle is drawn in javascript code\n","        # face_frame = img.copy()\n","        # cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","\n","        landmarks = predictor(gray, face)\n","        landmarks = face_utils.shape_to_np(landmarks)\n","\n","        # Eye landmarks are as follows:\n","        # Referenced from https://www.studytonight.com/post/dlib-68-points-face-landmark-detection-with-opencv-and-python\n","        left_blink = eye_aspect_ratio([landmarks[37],landmarks[38],\n","          landmarks[39], landmarks[40], landmarks[41], landmarks[42]])\n","        # print(\"left_blink :\", left_blink)\n","        right_blink = eye_aspect_ratio([landmarks[43],landmarks[44],\n","          landmarks[45], landmarks[46], landmarks[47], landmarks[48]])\n","        # print(\"right_blink: \", right_blink)\n","\n","        # Classify state based on EAR thresholds\n","        if left_blink < left_thresh and right_blink < right_thresh:\n","            current_status = 1 #\"Active\"\n","        elif left_blink > left_thresh and right_blink > right_thresh:\n","            current_status = 2 #\"Drowsy\"\n","        else:\n","            current_status = 3 #\"Uncertain\"\n","\n","    # # get face region coordinates\n","    # faces = face_cascade.detectMultiScale(gray)\n","    # # get face bounding box for overlay\n","    # for (x,y,w,h) in faces:\n","    #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","    status = current_status\n","    landmarks_J = landmarks.tolist()\n","\n"],"metadata":{"id":"50BMtGy2ZQyS","executionInfo":{"status":"ok","timestamp":1705071740791,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sipan Pal","userId":"00041362843998473830"}}},"execution_count":213,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HVCQ98pxZX_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R1-CjsL1ZZFI"},"execution_count":null,"outputs":[]}]}